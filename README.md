ğŸ“š Retrieval-Augmented Generation (RAG) Pipeline using LangChain + FAISS + OpenAI

This repository contains a Jupyter Notebook implementation of an end-to-end RAG pipeline built using:

ğŸ”¹ LangChain â€“ document loading, chunking, retrieval, prompting <br>
ğŸ”¹ FAISS â€“ vector indexing + similarity search <br>
ğŸ”¹ OpenAI â€“ embeddings + LLM generation <br>
ğŸ”¹ Python â€“ easy-to-follow step-based workflow <br>

This project demonstrates how Large Language Models can be grounded with retrieved knowledge to generate accurate, context-aware answers without hallucination.

ğŸ”¥ What this project does
| Step | Description | Tools Used |
| -------------------------- | ----------------------------------------------- | -------------------------------- |
| **1. Data Ingestion** | Reads PDF/Text/Markdown documents | LangChain Loaders |
| **2. Chunking** | Splits text into manageable overlapping chunks | `RecursiveCharacterTextSplitter` |
| **3. Embeddings** | Converts chunks into numerical vectors | `OpenAIEmbeddings` |
| **4. Vector Storage** | Indexes embeddings for similarity search | **FAISS** |
| **5. Retrieval** | Retrieves context relevant to a query | MMR / k-NN search |
| **6. Augmentation** | Injects retrieved chunks into prompt as context | `PromptTemplate` |
| **7. Generation** | Produces final grounded answer | `ChatOpenAI` |
| **8. Source Transparency** | Returns text chunks used for the answer | `return_source_documents=True` |

The entire flow is implemented inside a single .ipynb notebook for easy demonstration.

ğŸ“ Project Structure<br>
ğŸ“¦ RAG-LangChain-FAISS<br>
â”‚<br>
â”œâ”€â”€ data/ # Place your documents here<br>
â”œâ”€â”€ store/ # FAISS index will be saved/loaded here automatically<br>
â”œâ”€â”€ RAG_Pipeline.ipynb # Main notebook containing full implementation<br>
â””â”€â”€ README.md # (this file)<br>
The first run requires a document inside data/.<br>
If the folder is empty, the notebook automatically generates a sample file.

ğŸš€ Setup Instructions

1. Clone the project<br>
   git clone <repo-link> <br>
   cd RAG-LangChain-FAISS<br>

2. Create virtual environment (recommended)<br>
   python -m venv .venv<br>
   .venv\Scripts\activate # Windows<br>

3. Install dependencies<br>
   pip install -r requirements.txt

4. Add your OpenAI API key<br>
   setx OPENAI_API_KEY "sk-xxxxx"<br>

Restart terminal afterward.<br>

â–¶ Run the Notebook <br>
jupyter notebook <br>
Open RAG_Pipeline.ipynb<br>
Run each cell in order to execute the full pipeline.

ğŸ§  How the pipeline works â€“ Conceptual Flow<br>
ğŸ“„ Documents (.pdf/.txt/.md)<br>
â”‚<br>
[1] Load & Ingest<br>
â”‚<br>
[2] Split into Chunks<br>
â”‚<br>
[3] Create Embeddings (OpenAI)<br>
â”‚<br>
[4] Store in FAISS Vector Index<br>
â”‚<br>
User asks question<br>
â”‚<br>
[5] Retrieve Similar Chunks (MMR)<br>
â”‚<br>
[6] Augment Prompt w/ Context<br>
â”‚<br>
[7] Generate Answer (LLM)<br>
â†“<br>
ğŸ§© Final context-grounded answer<br>

ğŸŒŸ Key Features
âœ” No hallucinations â€” answers come from your data<br>
âœ” Local vector search (FAISS) for speed<br>
âœ” Works fully inside Jupyter (no backend server needed)<br>
âœ” Supports TXT, PDF, Markdown ingestion<br>
âœ” Transparent â€” prints retrieved context chunks<br>
âœ” Great starter template for production RAG systems<br>

ğŸ“Œ Ideal Use Cases<br>
ğŸ”¹QA on internal knowledge documents<br>
ğŸ”¹University/Policy/Manual based answering systems<br>
ğŸ”¹Company private knowledge bots<br>
ğŸ”¹PDF summarization + question answering<br>
ğŸ”¹Personal research assistant<br>
